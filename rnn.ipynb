{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent neural network for Google Stock Price\n",
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Data Preprocessing\n",
    "### Importing the training set\n",
    "\n",
    "Here, we are specifically importing and utilizing the training set in this analysis to highlight the fact that our model will be trained solely on this data. During the training phase, our model will have no knowledge of the test set, and there will be no equivalent of the test set available during training. Essentially, it's as if the test set doesn't exist for our model during the training process.\n",
    "\n",
    "However, once the training is completed, we will introduce the test set to assess and validate the model's performance by making predictions on future stock prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = pd.read_csv('./data/Google_Stock_Price_Train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/3/2012</td>\n",
       "      <td>325.25</td>\n",
       "      <td>332.83</td>\n",
       "      <td>324.97</td>\n",
       "      <td>663.59</td>\n",
       "      <td>7,380,500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/4/2012</td>\n",
       "      <td>331.27</td>\n",
       "      <td>333.87</td>\n",
       "      <td>329.08</td>\n",
       "      <td>666.45</td>\n",
       "      <td>5,749,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/5/2012</td>\n",
       "      <td>329.83</td>\n",
       "      <td>330.75</td>\n",
       "      <td>326.89</td>\n",
       "      <td>657.21</td>\n",
       "      <td>6,590,300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/6/2012</td>\n",
       "      <td>328.34</td>\n",
       "      <td>328.77</td>\n",
       "      <td>323.68</td>\n",
       "      <td>648.24</td>\n",
       "      <td>5,405,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/9/2012</td>\n",
       "      <td>322.04</td>\n",
       "      <td>322.29</td>\n",
       "      <td>309.46</td>\n",
       "      <td>620.76</td>\n",
       "      <td>11,688,800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Date    Open    High     Low   Close      Volume\n",
       "0  1/3/2012  325.25  332.83  324.97  663.59   7,380,500\n",
       "1  1/4/2012  331.27  333.87  329.08  666.45   5,749,400\n",
       "2  1/5/2012  329.83  330.75  326.89  657.21   6,590,300\n",
       "3  1/6/2012  328.34  328.77  323.68  648.24   5,405,900\n",
       "4  1/9/2012  322.04  322.29  309.46  620.76  11,688,800"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for missing values in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      "Date      0\n",
      "Open      0\n",
      "High      0\n",
      "Low       0\n",
      "Close     0\n",
      "Volume    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_values = dataset_train.isnull().sum()\n",
    "print(\"Missing values per column:\")\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have any missing values.\n",
    "\n",
    "Now we define the real data input for our model (training set) by selecting the necessary column (Open) and converting them into a NumPy array, which will serve as the input data for training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = dataset_train[['Open']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[325.25],\n",
       "       [331.27],\n",
       "       [329.83],\n",
       "       ...,\n",
       "       [793.7 ],\n",
       "       [783.33],\n",
       "       [782.75]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "Now, we are going to apply the appropriate feature scaling to our data to optimize the training process.\n",
    "\n",
    "We have two possibilities:\n",
    "- Standardization\n",
    "- Normalization\n",
    "\n",
    "I have chosen to use Normalization as it is more relevant in this context. When building an RNN, especially when a sigmoid function is used as an activation function in the output layer, it is recommended to apply normalization for improved performance.\n",
    "\n",
    "Normalization helps in bringing all features to a similar scale, which can aid in the training process by ensuring that no particular feature dominates due to its larger scale. This is particularly important for activation functions like sigmoid, where small input values can result in vanishing gradients, impacting learning during backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_scaled = scaler.fit_transform(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.08581368]\n",
      " [0.09701243]\n",
      " [0.09433366]\n",
      " ...\n",
      " [0.95725128]\n",
      " [0.93796041]\n",
      " [0.93688146]]\n"
     ]
    }
   ],
   "source": [
    "print(training_set_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a specific data structure\n",
    "Now, we will define a specific data structure that outlines what the RNN needs to remember when predicting the next stock price. This structure is referred to as the 'number of time steps.' It plays a critical role in determining the temporal memory or context the RNN will consider during its prediction of future stock prices.\n",
    "\n",
    "In this case, we have 60 timesteps and one output. This implies that at each time 't,' the RNN will analyze the 60 stock prices leading up to time 't' (or the 60 days prior to time 't'), and then we will attempt to predict the subsequent output.\n",
    "\n",
    "X_train: The input for the RNN, consisting of the 60 previous stock prices.\n",
    "y_train: The output representing the stock price for the next financial day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "nb_timesteps = 60\n",
    "\n",
    "for i in range(nb_timesteps, len(training_set_scaled)):\n",
    "    X_train.append(training_set_scaled[i-nb_timesteps:i, 0])\n",
    "    y_train.append(training_set_scaled[i,0])\n",
    "\n",
    "X_train, y_train = np.array(X_train), np.array(y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.08581368 0.09701243 0.09433366 ... 0.07846566 0.08034452 0.08497656]\n",
      " [0.09701243 0.09433366 0.09156187 ... 0.08034452 0.08497656 0.08627874]\n",
      " [0.09433366 0.09156187 0.07984225 ... 0.08497656 0.08627874 0.08471612]\n",
      " ...\n",
      " [0.92106928 0.92438053 0.93048218 ... 0.95475854 0.95204256 0.95163331]\n",
      " [0.92438053 0.93048218 0.9299055  ... 0.95204256 0.95163331 0.95725128]\n",
      " [0.93048218 0.9299055  0.93113327 ... 0.95163331 0.95725128 0.93796041]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08627874 0.08471612 0.07454052 ... 0.95725128 0.93796041 0.93688146]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping \n",
    "We are now going to reshape the data structure to introduce additional dimensions to the previous data structure, allowing for the inclusion of more indicators if desired.\n",
    "\n",
    "The input shape with Keras should be a 3D tensor with dimensions (batch_size, timesteps, input_dim) for Recurrent Layers. 'Batch_size' corresponds to the number of observations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, timesteps = X_train.shape\n",
    "input_dim = 1\n",
    "\n",
    "X_train = np.reshape(X_train, (batch_size, timesteps, input_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the right structure expected for our RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Building the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will construct a robust architecture by not only using a simple LSTM but also implementing a stacked LSTM with dropout regularization to prevent overfitting.\n",
    "\n",
    "### Initialising the RNN\n",
    "Initialising the RNN as a sequence of layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the first LSTM layer and some Dropout regularisation\n",
    "As mentionned before, we will use some dropout regularization, but what's it ? \n",
    "\n",
    "This is a technique used in neural network training to prevent overfitting and improve the model's generalization performance. During the training process, dropout randomly sets a fraction (rate) of the neurons in a layer to zero, effectively 'dropping out' those units. This means that the model trains on a reduced network for each batch, as different neurons are dropped out in each training iteration.\n",
    "\n",
    "By doing this, dropout helps prevent the neural network from relying too heavily on a specific set of neurons and encourages the network to learn more robust and generalizable features. It essentially forces the model to learn redundant representations of information, reducing the risk of overfitting to the training data.\n",
    "\n",
    "When utilizing multiple LSTM layers, it is necessary to set 'return_sequences' to True. 'Units' represents the number of LSTM cells, memory units, or neurons that we intend to have in this initial LSTM layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1],1)))\n",
    "regressor.add(Dropout(rate=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding others LSTM layers with Dropout regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.add(LSTM(units=50, return_sequences=True))\n",
    "regressor.add(Dropout(rate=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.add(LSTM(units=50, return_sequences=True))\n",
    "regressor.add(Dropout(rate=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.add(LSTM(units=50))\n",
    "regressor.add(Dropout(rate=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.add(Dense(units=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.compile(optimizer=\"adam\", loss=\"mean_squared_error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the RNN on the training set\n",
    "Epochs represent the number of times the entire dataset is used for training. With 100 epochs, the model learns from the data 100 times.\n",
    "\n",
    "Batch size refers to the number of data samples processed in a single training iteration. A batch size of 32 means 32 samples are used to update the model's weights at each step, enhancing training efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/85\n",
      "38/38 [==============================] - 10s 65ms/step - loss: 0.0389\n",
      "Epoch 2/85\n",
      "38/38 [==============================] - 2s 65ms/step - loss: 0.0060\n",
      "Epoch 3/85\n",
      "38/38 [==============================] - 3s 68ms/step - loss: 0.0051\n",
      "Epoch 4/85\n",
      "38/38 [==============================] - 3s 87ms/step - loss: 0.0049\n",
      "Epoch 5/85\n",
      "38/38 [==============================] - 3s 89ms/step - loss: 0.0050\n",
      "Epoch 6/85\n",
      "38/38 [==============================] - 3s 89ms/step - loss: 0.0050\n",
      "Epoch 7/85\n",
      "38/38 [==============================] - 3s 89ms/step - loss: 0.0046\n",
      "Epoch 8/85\n",
      "38/38 [==============================] - 3s 91ms/step - loss: 0.0048\n",
      "Epoch 9/85\n",
      "38/38 [==============================] - 3s 90ms/step - loss: 0.0045\n",
      "Epoch 10/85\n",
      "38/38 [==============================] - 3s 89ms/step - loss: 0.0044\n",
      "Epoch 11/85\n",
      "38/38 [==============================] - 3s 89ms/step - loss: 0.0048\n",
      "Epoch 12/85\n",
      "38/38 [==============================] - 3s 90ms/step - loss: 0.0042\n",
      "Epoch 13/85\n",
      "38/38 [==============================] - 3s 89ms/step - loss: 0.0039\n",
      "Epoch 14/85\n",
      "38/38 [==============================] - 3s 89ms/step - loss: 0.0041\n",
      "Epoch 15/85\n",
      "38/38 [==============================] - 3s 91ms/step - loss: 0.0037\n",
      "Epoch 16/85\n",
      "38/38 [==============================] - 3s 88ms/step - loss: 0.0037\n",
      "Epoch 17/85\n",
      "38/38 [==============================] - 3s 89ms/step - loss: 0.0035\n",
      "Epoch 18/85\n",
      "38/38 [==============================] - 3s 88ms/step - loss: 0.0041\n",
      "Epoch 19/85\n",
      "38/38 [==============================] - 3s 89ms/step - loss: 0.0045\n",
      "Epoch 20/85\n",
      "38/38 [==============================] - 3s 89ms/step - loss: 0.0038\n",
      "Epoch 21/85\n",
      "38/38 [==============================] - 3s 91ms/step - loss: 0.0041\n",
      "Epoch 22/85\n",
      "38/38 [==============================] - 3s 81ms/step - loss: 0.0032\n",
      "Epoch 23/85\n",
      "38/38 [==============================] - 3s 67ms/step - loss: 0.0034\n",
      "Epoch 24/85\n",
      "38/38 [==============================] - 3s 67ms/step - loss: 0.0032\n",
      "Epoch 25/85\n",
      "38/38 [==============================] - 3s 66ms/step - loss: 0.0032\n",
      "Epoch 26/85\n",
      "38/38 [==============================] - 3s 66ms/step - loss: 0.0034\n",
      "Epoch 27/85\n",
      "38/38 [==============================] - 2s 65ms/step - loss: 0.0038\n",
      "Epoch 28/85\n",
      "38/38 [==============================] - 2s 64ms/step - loss: 0.0031\n",
      "Epoch 29/85\n",
      "38/38 [==============================] - 2s 55ms/step - loss: 0.0029\n",
      "Epoch 30/85\n",
      "38/38 [==============================] - 3s 72ms/step - loss: 0.0030\n",
      "Epoch 31/85\n",
      "38/38 [==============================] - 3s 76ms/step - loss: 0.0029\n",
      "Epoch 32/85\n",
      "38/38 [==============================] - 2s 65ms/step - loss: 0.0029\n",
      "Epoch 33/85\n",
      "38/38 [==============================] - 2s 65ms/step - loss: 0.0028\n",
      "Epoch 34/85\n",
      "38/38 [==============================] - 2s 65ms/step - loss: 0.0029\n",
      "Epoch 35/85\n",
      "38/38 [==============================] - 3s 68ms/step - loss: 0.0027\n",
      "Epoch 36/85\n",
      "38/38 [==============================] - 3s 67ms/step - loss: 0.0029\n",
      "Epoch 37/85\n",
      "38/38 [==============================] - 3s 68ms/step - loss: 0.0028\n",
      "Epoch 38/85\n",
      "38/38 [==============================] - 3s 67ms/step - loss: 0.0027\n",
      "Epoch 39/85\n",
      "38/38 [==============================] - 3s 67ms/step - loss: 0.0030\n",
      "Epoch 40/85\n",
      "38/38 [==============================] - 3s 66ms/step - loss: 0.0027\n",
      "Epoch 41/85\n",
      "38/38 [==============================] - 3s 67ms/step - loss: 0.0025\n",
      "Epoch 42/85\n",
      "38/38 [==============================] - 3s 67ms/step - loss: 0.0024\n",
      "Epoch 43/85\n",
      "38/38 [==============================] - 3s 68ms/step - loss: 0.0027\n",
      "Epoch 44/85\n",
      "38/38 [==============================] - 3s 67ms/step - loss: 0.0022\n",
      "Epoch 45/85\n",
      "38/38 [==============================] - 3s 66ms/step - loss: 0.0025\n",
      "Epoch 46/85\n",
      "38/38 [==============================] - 3s 68ms/step - loss: 0.0028\n",
      "Epoch 47/85\n",
      "38/38 [==============================] - 2s 64ms/step - loss: 0.0027\n",
      "Epoch 48/85\n",
      "38/38 [==============================] - 2s 65ms/step - loss: 0.0022\n",
      "Epoch 49/85\n",
      "38/38 [==============================] - 3s 70ms/step - loss: 0.0023\n",
      "Epoch 50/85\n",
      "38/38 [==============================] - 3s 68ms/step - loss: 0.0023\n",
      "Epoch 51/85\n",
      "38/38 [==============================] - 3s 67ms/step - loss: 0.0023\n",
      "Epoch 52/85\n",
      "38/38 [==============================] - 3s 69ms/step - loss: 0.0026\n",
      "Epoch 53/85\n",
      "38/38 [==============================] - 3s 70ms/step - loss: 0.0022\n",
      "Epoch 54/85\n",
      "38/38 [==============================] - 3s 69ms/step - loss: 0.0025\n",
      "Epoch 55/85\n",
      "38/38 [==============================] - 3s 66ms/step - loss: 0.0023\n",
      "Epoch 56/85\n",
      "38/38 [==============================] - 3s 67ms/step - loss: 0.0021\n",
      "Epoch 57/85\n",
      "38/38 [==============================] - 3s 82ms/step - loss: 0.0021\n",
      "Epoch 58/85\n",
      "38/38 [==============================] - 3s 91ms/step - loss: 0.0022\n",
      "Epoch 59/85\n",
      "38/38 [==============================] - 3s 92ms/step - loss: 0.0022\n",
      "Epoch 60/85\n",
      "38/38 [==============================] - 3s 92ms/step - loss: 0.0021\n",
      "Epoch 61/85\n",
      "38/38 [==============================] - 3s 79ms/step - loss: 0.0021\n",
      "Epoch 62/85\n",
      "38/38 [==============================] - 3s 67ms/step - loss: 0.0022\n",
      "Epoch 63/85\n",
      "38/38 [==============================] - 2s 66ms/step - loss: 0.0021\n",
      "Epoch 64/85\n",
      "38/38 [==============================] - 3s 67ms/step - loss: 0.0020\n",
      "Epoch 65/85\n",
      "38/38 [==============================] - 3s 67ms/step - loss: 0.0021\n",
      "Epoch 66/85\n",
      "38/38 [==============================] - 3s 68ms/step - loss: 0.0019\n",
      "Epoch 67/85\n",
      "38/38 [==============================] - 3s 68ms/step - loss: 0.0022\n",
      "Epoch 68/85\n",
      "38/38 [==============================] - 3s 88ms/step - loss: 0.0020\n",
      "Epoch 69/85\n",
      "38/38 [==============================] - 3s 92ms/step - loss: 0.0018\n",
      "Epoch 70/85\n",
      "38/38 [==============================] - 3s 91ms/step - loss: 0.0020\n",
      "Epoch 71/85\n",
      "38/38 [==============================] - 4s 92ms/step - loss: 0.0018\n",
      "Epoch 72/85\n",
      "38/38 [==============================] - 3s 92ms/step - loss: 0.0019\n",
      "Epoch 73/85\n",
      "38/38 [==============================] - 3s 92ms/step - loss: 0.0018\n",
      "Epoch 74/85\n",
      "38/38 [==============================] - 3s 91ms/step - loss: 0.0017\n",
      "Epoch 75/85\n",
      "38/38 [==============================] - 4s 92ms/step - loss: 0.0018\n",
      "Epoch 76/85\n",
      "38/38 [==============================] - 3s 91ms/step - loss: 0.0020\n",
      "Epoch 77/85\n",
      "38/38 [==============================] - 3s 91ms/step - loss: 0.0022\n",
      "Epoch 78/85\n",
      "38/38 [==============================] - 4s 92ms/step - loss: 0.0016\n",
      "Epoch 79/85\n",
      "38/38 [==============================] - 3s 92ms/step - loss: 0.0018\n",
      "Epoch 80/85\n",
      "38/38 [==============================] - 3s 84ms/step - loss: 0.0015\n",
      "Epoch 81/85\n",
      "38/38 [==============================] - 3s 85ms/step - loss: 0.0016\n",
      "Epoch 82/85\n",
      "38/38 [==============================] - 3s 91ms/step - loss: 0.0016\n",
      "Epoch 83/85\n",
      "38/38 [==============================] - 4s 92ms/step - loss: 0.0015\n",
      "Epoch 84/85\n",
      "38/38 [==============================] - 3s 91ms/step - loss: 0.0015\n",
      "Epoch 85/85\n",
      "38/38 [==============================] - 3s 91ms/step - loss: 0.0017\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x20e7da25fd0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.fit(x=X_train, y=y_train, epochs=85, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fitting process appears to converge around the 80th epoch, suggesting that the model's performance stabilizes and further epochs may not significantly enhance performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Making the predictions and visualising the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
